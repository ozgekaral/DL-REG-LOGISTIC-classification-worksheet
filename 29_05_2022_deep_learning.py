# -*- coding: utf-8 -*-
"""29_05_2022_Deep_Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dc7m6DKUSEK0GalL32rnaZWZxK9bWnH0
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

"""## Deep Learning for Regression"""

from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split

data = load_diabetes()

X = data["data"]
y = data["target"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

mu = X_train.mean(axis = 0)
sigma = X_train.std(axis = 0)

X_train = (X_train - mu) / sigma
X_test = (X_test - mu) / sigma
X_train.shape

mdl = keras.Sequential()

mdl.add(layers.Dense(128, activation="relu", input_shape = (X_train.shape[1], ) ))
mdl.add(layers.Dense(64, activation = "relu"))
mdl.add(layers.Dense(1, activation = "linear"))

mdl.compile(optimizer="adam", loss = "mse", metrics = ["mape"])
mdl.fit(X_train, y_train, epochs=100)

ypred = mdl.predict(X_test)
plt.plot(y_test, ypred, "*")

mdl.evaluate(X_test,y_test)

"""## Deep Learning for Binary Classification"""

from sklearn.datasets import load_breast_cancer

data = load_breast_cancer()

X = data["data"]
y = data["target"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

mu = X_train.mean(axis = 0)
sigma = X_train.std(axis = 0)

X_train = (X_train - mu) / sigma
X_test = (X_test - mu) / sigma
X_train.shape

mdl = keras.Sequential()

mdl.add(layers.Dense(128, activation="relu", input_shape = (X_train.shape[1], ) ))
mdl.add(layers.Dense(64, activation = "relu"))
mdl.add(layers.Dense(1, activation = "sigmoid"))

mdl.compile(optimizer="adam", loss = "binary_crossentropy", metrics = ["accuracy"])
mdl.fit(X_train, y_train, epochs=100)

ypred = mdl.predict(X_test)
ypred_label = (ypred > 0.5).astype("int")
ypred_label

mdl.evaluate(X_test, y_test)

"""## Deep Learning for Multinomial Classification


"""

from sklearn.datasets import load_iris

data = load_iris()

X = data["data"]
y = data["target"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

mu = X_train.mean(axis = 0)
sigma = X_train.std(axis = 0)

X_train = (X_train - mu) / sigma
X_test = (X_test - mu) / sigma
X_train.shape

mdl = keras.Sequential()

mdl.add(layers.Dense(128, activation="relu", input_shape = (X_train.shape[1], ) ))
mdl.add(layers.Dense(64, activation = "relu"))
mdl.add(layers.Dense(3, activation = "softmax"))

mdl.compile(optimizer="adam", loss = "sparse_categorical_crossentropy", metrics = ["accuracy"])
mdl.fit(X_train, y_train, epochs=100)

ypred = mdl.predict(X_test)
ypred_label = np.argmax(ypred, axis = 1)
ypred_label

mdl.evaluate(X_test,y_test)

"""## Overfitting in Deep Learning"""

from sklearn.datasets import load_boston

data = load_boston()

X = data["data"]
y = data["target"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

mu = X_train.mean(axis = 0)
sigma = X_train.std(axis = 0)

X_train = (X_train - mu) / sigma
X_test = (X_test - mu) / sigma
X_train.shape

mdl = keras.Sequential()

mdl.add(layers.Dense(512, activation="relu", input_shape = (X_train.shape[1], ) ))
mdl.add(layers.Dropout(0.2))
mdl.add(layers.Dense(256, activation = "relu"))
mdl.add(layers.Dropout(0.2))
mdl.add(layers.Dense(128, activation = "relu"))
mdl.add(layers.Dropout(0.2))
mdl.add(layers.Dense(64, activation = "relu"))
mdl.add(layers.Dense(1, activation = "linear"))

early_stop = keras.callbacks.EarlyStopping(monitor = "val_loss", patience=30)

mdl.compile(optimizer="adam", loss = "mse", metrics = ["mape"])
history = mdl.fit(X_train, y_train, epochs=1000, verbose = 0, validation_split=0.1, callbacks=[early_stop])
mdl.evaluate(X_train, y_train)
mdl.evaluate(X_test, y_test)

plt.figure(figsize = (12,4))
plt.subplot(1,2,1)
plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.ylim([0,20])

plt.subplot(1,2,2)
plt.plot(history.history["mape"])
plt.plot(history.history["val_mape"])
plt.ylim([0,20])

"""## Multi Layer Perceptron for Image Analysis"""

(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()

X_train.shape

labels = ["T-shirt", "Trouser", "Pullover", "Dress", "Coat", "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]

plt.imshow(X_train[0], "gray")
print(labels[y_train[0]])

X_train = X_train.reshape(-1,28*28)
X_test = X_test.reshape(-1,28*28)

X_train = X_train / 255.0
X_test = X_test / 255.0

mdl = keras.Sequential()

mdl.add(layers.Dense(128, activation="relu", input_shape = (X_train.shape[1], ) ))
mdl.add(layers.Dense(64, activation = "relu"))
mdl.add(layers.Dense(10, activation = "softmax"))

mdl.compile(optimizer="adam", loss = "sparse_categorical_crossentropy", metrics = ["accuracy"])
mdl.fit(X_train, y_train, epochs=10)

mdl.evaluate(X_test, y_test)

(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()

X_train = X_train / 255.0
X_test = X_test / 255.0
X_train.shape

mdl = keras.Sequential()

mdl.add(layers.Flatten(input_shape = (28,28)))
mdl.add(layers.Dense(128, activation="relu"))
mdl.add(layers.Dense(64, activation = "relu"))
mdl.add(layers.Dense(10, activation = "softmax"))

mdl.compile(optimizer="adam", loss = "sparse_categorical_crossentropy", metrics = ["accuracy"])
mdl.fit(X_train, y_train, epochs=10)

"""## Convolutional Neural Networks"""

(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()


X_train = X_train[..., np.newaxis]
X_test = X_test[..., np.newaxis]

X_train = X_train / 255.0
X_test = X_test / 255.0
X_train.shape

mdl = keras.Sequential()

mdl.add(layers.Conv2D(128, kernel_size = (3,3), activation="relu", input_shape = X_train.shape[1:] ))
mdl.add(layers.MaxPool2D(pool_size=(2,2)))
mdl.add(layers.Conv2D(64, kernel_size = (3,3), activation="relu" ))
mdl.add(layers.MaxPool2D(pool_size=(2,2)))
mdl.add(layers.Flatten())
mdl.add(layers.Dense(128, activation="relu"))
mdl.add(layers.Dense(64, activation = "relu"))
mdl.add(layers.Dense(10, activation = "softmax"))

mdl.compile(optimizer="adam", loss = "sparse_categorical_crossentropy", metrics = ["accuracy"])
mdl.fit(X_train, y_train, epochs=10)

from PIL import Image, ImageOps

test = Image.open("test1.png")
test = ImageOps.grayscale(test)
test = test.resize((28,28))
test_array = np.asarray(test)
plt.imshow(test_array, "gray")
test_array = test_array.reshape(1,28,28,1)
test_array = test_array / 255.0
labels[np.argmax(mdl.predict(test_array))]

ypred = mdl.predict(X_test)
ypred_label = np.argmax(ypred, axis = 1)

np.argmax(ypred, axis = 1)

plt.figure(figsize = (16,8))

for i in range(10):
    plt.subplot(2,5, i + 1)
    plt.imshow(X_test[i].reshape(28,28), "gray")
    plt.title(labels[ypred_label[i]])
    plt.axis("off")

df = pd.read_excel("natural_gas.xlsx")
df.head()

consumption = df["Consumption"]

def make_data(data, lag = 3):
    X = []
    y = []
    for i in range(len(data)-lag):
        X.append(data[i:i+lag])
        y.append(data[i+lag])

    return np.array(X), np.array(y)

X, y = make_data(consumption, lag = 12)
X.shape

X_train = X[:-12]
y_train = y[:-12]

X_test = X[-12:]
y_test = y[-12:]

mu = y_train.mean()
sigma = y_train.std()

X_train = (X_train - mu) / sigma
y_train = (y_train - mu) / sigma

X_test = (X_test - mu) / sigma
y_test = (y_test - mu) / sigma

X_train = X_train[..., np.newaxis]
X_test = X_test[..., np.newaxis]
X_train.shape

mdl = keras.Sequential()

mdl.add(layers.LSTM(64, input_shape = X_train.shape[1:]))
mdl.add(layers.Dense(64, activation="relu"))
mdl.add(layers.Dense(1))

mdl.compile(optimizer="adam", loss = "mse", metrics = ["mae"])
mdl.fit(X_train, y_train, epochs=100)

ypred = mdl.predict(X_test)

plt.plot(y_test, "-ob", label = "observed")
plt.plot(ypred, "-*r", label = "forecasted")
plt.grid()
plt.legend()

(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()

X_train = X_train.reshape(-1,28*28)
X_test = X_test.reshape(-1,28*28)

X_train = X_train / 255.0
X_test = X_test / 255.0
X_train.shape

mdl = keras.Sequential()


mdl.add(layers.Dense(512, activation="relu", input_shape = (784, )))
mdl.add(layers.Dense(256, activation="relu"))
mdl.add(layers.Dense(512, activation="relu"))
mdl.add(layers.Dense(28*28, activation="sigmoid"))

mdl.compile(optimizer="adam", loss = "mse", metrics = ["mae"])
mdl.fit(X_train, X_train, epochs=10)

X_test_reconstructed = mdl.predict(X_test)

plt.figure(figsize = (16,8))

for i in range(5):
    plt.subplot(2,5, i + 1)
    plt.imshow(X_test[i].reshape(28,28), "gray")
    plt.title("Original")
    plt.axis("off")

    plt.subplot(2,5, i + 5 + 1)
    plt.imshow(X_test_reconstructed[i].reshape(28,28), "gray")
    plt.title("Reconstructed")
    plt.axis("off")

noise_factor = 0.2

X_train_noisy = X_train + noise_factor * np.random.normal(size = X_train.shape)
X_test_noisy = X_test + noise_factor * np.random.normal(size = X_test.shape)

X_train_noisy = np.clip(X_train_noisy, 0,1)
X_test_noisy = np.clip(X_test_noisy, 0,1)

plt.figure(figsize = (12,6))

for i in range(5):
    plt.subplot(1,5, i + 1)
    plt.imshow(X_train_noisy[i].reshape(28,28), "gray")
    plt.title("Original")
    plt.axis("off")

mdl = keras.Sequential()


mdl.add(layers.Dense(512, activation="relu", input_shape = (784, )))
mdl.add(layers.Dense(256, activation="relu"))
mdl.add(layers.Dense(512, activation="relu"))
mdl.add(layers.Dense(28*28, activation="sigmoid"))

mdl.compile(optimizer="adam", loss = "mse", metrics = ["mae"])
mdl.fit(X_train_noisy, X_train, epochs=10)

X_test_reconstructed = mdl.predict(X_test_noisy)

plt.figure(figsize = (16,8))

for i in range(5):
    plt.subplot(2,5, i + 1)
    plt.imshow(X_test_noisy[i].reshape(28,28), "gray")
    plt.title("Noisy")
    plt.axis("off")

    plt.subplot(2,5, i + 5 + 1)
    plt.imshow(X_test_reconstructed[i].reshape(28,28), "gray")
    plt.title("Reconstructed")
    plt.axis("off")